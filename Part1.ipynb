{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tUk0z5kjNUP"
      },
      "source": [
        "## Part 1: Language Modeling\n",
        "\n",
        "In this project, you will implement several different types of language models for text.  We'll start with n-gram models and then move on to neural n-gram.\n",
        "\n",
        "Warning: Do not start this project the day before it is due!  Some parts require 20 minutes or more to run, so debugging and tuning can take a significant amount of time.\n",
        "\n",
        "Our dataset for this project will be the Penn Treebank language modeling dataset.  This dataset comes with some of the basic preprocessing done for us, such as tokenization and rare word filtering (using the `<unk>` token).\n",
        "Therefore, we can assume that all word types in the test set also appear at least once in the training set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-V_KFhojlz5",
        "outputId": "3c6b5972-b370-462a-bb60-4bb0ff285e6b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "X1ATX-_J7SjQ",
        "outputId": "1a08dcb1-1acd-4e73-f7bc-3b8e5d7efb18",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './drive/MyDrive/CS-288'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2556312615.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# The arrow files are in ./wikitext/wikitext-2-v1/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_LAUNCH_BLOCKING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./drive/MyDrive/CS-288'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mcache_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./wikitext/wikitext-2-v1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './drive/MyDrive/CS-288'"
          ]
        }
      ],
      "source": [
        "# This block handles some basic setup and data loading.\n",
        "# You shouldn't need to edit this, but if you want to\n",
        "# import other standard python packages, that is fine.\n",
        "\n",
        "# imports\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import math\n",
        "import tqdm\n",
        "import random\n",
        "import pdb\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from datasets import Dataset\n",
        "import os\n",
        "\n",
        "# Load WikiText-2 dataset from local arrow files\n",
        "# The arrow files are in ./wikitext/wikitext-2-v1/\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.chdir('./drive/MyDrive/CS-288')\n",
        "cache_path = \"./wikitext/wikitext-2-v1\"\n",
        "\n",
        "# Load the three splits from arrow files using Dataset.from_file()\n",
        "train_dataset = Dataset.from_file(os.path.join(cache_path, \"wikitext-train.arrow\"))\n",
        "validation_dataset = Dataset.from_file(os.path.join(cache_path, \"wikitext-validation.arrow\"))\n",
        "test_dataset = Dataset.from_file(os.path.join(cache_path, \"wikitext-test.arrow\"))\n",
        "\n",
        "# Convert to list of tokens (HuggingFace returns text as strings, we need to tokenize)\n",
        "# WikiText-2 is already tokenized with space-separated tokens\n",
        "def get_tokens(example):\n",
        "    # Split by whitespace and filter out empty strings\n",
        "    tokens = example['text'].split()\n",
        "    return tokens\n",
        "\n",
        "# Get all tokens from each split\n",
        "train_text = []\n",
        "for example in train_dataset:\n",
        "    tokens = get_tokens(example)\n",
        "    if tokens:  # Skip empty examples\n",
        "        train_text.extend(tokens)\n",
        "\n",
        "validation_text = []\n",
        "for example in validation_dataset:\n",
        "    tokens = get_tokens(example)\n",
        "    if tokens:\n",
        "        validation_text.extend(tokens)\n",
        "\n",
        "test_text = []\n",
        "for example in test_dataset:\n",
        "    tokens = get_tokens(example)\n",
        "    if tokens:\n",
        "        test_text.extend(tokens)\n",
        "\n",
        "# Build vocabulary from training set only\n",
        "# (Validation and test sets may contain unknown tokens, which will be mapped to <unk>)\n",
        "token_counts = Counter(train_text)\n",
        "\n",
        "# Create vocabulary with special tokens\n",
        "# Add special tokens: <unk> for unknown words, <eos> for end of sentence\n",
        "special_tokens = ['<unk>', '<eos>', '<pad>']\n",
        "for token in special_tokens:\n",
        "    token_counts[token] = 0\n",
        "vocab_list = sorted(token_counts.keys())\n",
        "vocab_size = len(vocab_list)\n",
        "\n",
        "# Create a simple vocab class compatible with torchtext interface\n",
        "class Vocab:\n",
        "    def __init__(self, vocab_list, token_counts):\n",
        "        self.itos = vocab_list  # index to string\n",
        "        self.stoi = {word: idx for idx, word in enumerate(vocab_list)}  # string to index\n",
        "        self.freqs = token_counts  # frequency counts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "vocab = Vocab(vocab_list, token_counts)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"First 30 validation tokens: {validation_text[:30]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QoFMSckDFTop",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28fe9fd0-7f3c-48bc-c205-64206affa1cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['=', 'Homarus', 'gammarus', '=', 'Homarus', 'gammarus', ',', 'known', 'as', 'the', 'European', 'lobster', 'or', 'common', 'lobster', ',', 'is', 'a', 'species', 'of', '<unk>', 'lobster', 'from', 'the', 'eastern', 'Atlantic', 'Ocean', ',', 'Mediterranean', 'Sea', 'and', 'parts', 'of', 'the', 'Black', 'Sea', '.', 'It', 'is', 'closely', 'related', 'to', 'the', 'American', 'lobster', ',', 'H.', 'americanus', '.', 'It', 'may', 'grow', 'to', 'a', 'length', 'of', '60', 'cm', '(', '24', 'in', ')', 'and', 'a', 'mass', 'of', '6', 'kilograms', '(', '13', 'lb', ')', ',', 'and', 'bears', 'a', 'conspicuous', 'pair', 'of', 'claws', '.', 'In', 'life', ',', 'the', 'lobsters', 'are', 'blue', ',', 'only', 'becoming', '\"', 'lobster', 'red', '\"', 'on', 'cooking', '.', 'Mating', 'occurs', 'in', 'the', 'summer', ',', 'producing', 'eggs', 'which', 'are', 'carried', 'by', 'the', 'females', 'for', 'up', 'to', 'a', 'year', 'before', 'hatching', 'into', '<unk>', 'larvae', '.', 'Homarus', 'gammarus', 'is', 'a', 'highly', 'esteemed', 'food', ',', 'and', 'is', 'widely', 'caught', 'using', 'lobster', 'pots', ',', 'mostly', 'around', 'the', 'British', 'Isles', '.', '=', '=', 'Description', '=', '=', 'Homarus', 'gammarus', 'is', 'a', 'large', '<unk>', ',', 'with', 'a', 'body', 'length', 'up', 'to', '60', 'centimetres', '(', '24', 'in', ')', 'and', 'weighing', 'up', 'to', '5', '–', '6', 'kilograms', '(', '11', '–', '13', 'lb', ')', ',', 'although', 'the', 'lobsters', 'caught', 'in', 'lobster', 'pots', 'are', 'usually', '23', '–', '38', 'cm', '(', '9', '–', '15', 'in', ')', 'long', 'and', 'weigh', '0', '@.@', '7', '–', '2', '@.@', '2', 'kg', '(', '1', '@.@', '5', '–', '4', '@.@', '9', 'lb', ')', '.', 'Like', 'other', 'crustaceans', ',', 'lobsters', 'have', 'a', 'hard', '<unk>', 'which', 'they', 'must', 'shed', 'in', 'order', 'to', 'grow', ',', 'in', 'a', 'process', 'called', '<unk>', '(', '<unk>', ')', '.', 'This', 'may', 'occur', 'several', 'times', 'a', 'year', 'for', 'young', 'lobsters', ',', 'but', 'decreases', 'to', 'once', 'every', '1', '–', '2', 'years', 'for', 'larger', 'animals', '.', 'The', 'first', 'pair', 'of', '<unk>', 'is', 'armed', 'with', 'a', 'large', ',', 'asymmetrical', 'pair', 'of', 'claws', '.', 'The', 'larger', 'one', 'is', 'the', '\"', '<unk>', '\"']\n"
          ]
        }
      ],
      "source": [
        "print(validation_text[:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g10PLGiZn0XY"
      },
      "source": [
        "We've implemented a unigram model here as a demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B7ZHMVZzoPEH",
        "outputId": "ce412e21-6a71-45c6-9de5-255c6f91ba49",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram validation perplexity: 996.5031614073108\n"
          ]
        }
      ],
      "source": [
        "class UnigramModel:\n",
        "    def __init__(self, train_text):\n",
        "        self.counts = Counter(train_text)\n",
        "        self.total_count = len(train_text)\n",
        "\n",
        "    def probability(self, word):\n",
        "        return self.counts[word] / self.total_count\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        \"\"\"Return a list of probabilities for each word in the vocabulary.\"\"\"\n",
        "        return [self.probability(word) for word in vocab.itos]\n",
        "\n",
        "    def perplexity(self, full_text):\n",
        "        \"\"\"Return the perplexity of the model on a text as a float.\n",
        "\n",
        "        full_text -- a list of string tokens\n",
        "        \"\"\"\n",
        "        log_probabilities = []\n",
        "        for word in full_text:\n",
        "            # Note that the base of the log doesn't matter\n",
        "            # as long as the log and exp use the same base.\n",
        "            prob = self.probability(word)\n",
        "            # Handle 0 probability by using a very small epsilon to avoid log(0)\n",
        "            if prob == 0:\n",
        "                prob = 1e-10\n",
        "            log_probabilities.append(math.log(prob, 2))\n",
        "        return 2 ** -np.mean(log_probabilities)\n",
        "\n",
        "unigram_demonstration_model = UnigramModel(train_text)\n",
        "print('unigram validation perplexity:',\n",
        "      unigram_demonstration_model.perplexity(validation_text))\n",
        "\n",
        "def check_validity(model):\n",
        "    \"\"\"Performs several sanity checks on your model:\n",
        "    1) That next_word_probabilities returns a valid distribution\n",
        "    2) That perplexity matches a perplexity calculated from next_word_probabilities\n",
        "\n",
        "    Although it is possible to calculate perplexity from next_word_probabilities,\n",
        "    it is still good to have a separate more efficient method that only computes\n",
        "    the probabilities of observed words.\n",
        "    \"\"\"\n",
        "\n",
        "    log_probabilities = []\n",
        "    for i in range(10):\n",
        "        prefix = validation_text[:i]\n",
        "        probs = model.next_word_probabilities(prefix)\n",
        "        assert min(probs) >= 0, \"Negative value in next_word_probabilities\"\n",
        "        assert max(probs) <= 1 + 1e-8, \"Value larger than 1 in next_word_probabilities\"\n",
        "        assert abs(sum(probs)-1) < 1e-4, \"next_word_probabilities do not sum to 1\"\n",
        "\n",
        "        word_id = vocab.stoi[validation_text[i]]\n",
        "        selected_prob = probs[word_id]\n",
        "        # Handle 0 probability by using a very small epsilon to avoid log(0)\n",
        "        if selected_prob == 0:\n",
        "            selected_prob = 1e-10\n",
        "        log_probabilities.append(math.log(selected_prob, 2))\n",
        "\n",
        "    perplexity = 2 ** -np.mean(log_probabilities)\n",
        "    your_perplexity = model.perplexity(validation_text[:10])\n",
        "    assert abs(perplexity-your_perplexity) < 0.1, \"your perplexity does not \" + \\\n",
        "    \"match the one we calculated from `next_word_probabilities`,\\n\" + \\\n",
        "    \"at least one of `perplexity` or `next_word_probabilities` is incorrect.\\n\" + \\\n",
        "    f\"we calcuated {perplexity} from `next_word_probabilities`,\\n\" + \\\n",
        "    f\"but your perplexity function returned {your_perplexity} (on a small sample).\"\n",
        "\n",
        "\n",
        "check_validity(unigram_demonstration_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4esz5XrEpNo"
      },
      "source": [
        "To generate from a language model, we can sample one word at a time conditioning on the words we have generated so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bfNj5nl4E7Zn",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d481641-e6e4-4910-caca-7a114c40cd9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<eos> <eos> , , above to one ) February inadequate his for a Circuit manipulating Children upstream with a afterlife Secondary suggestions\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, n=20, prefix=('<eos>', '<eos>')):\n",
        "    prefix = list(prefix)\n",
        "    for _ in range(n):\n",
        "        probs = model.next_word_probabilities(prefix)\n",
        "        word = random.choices(vocab.itos, probs)[0]\n",
        "        prefix.append(word)\n",
        "    return ' '.join(prefix)\n",
        "\n",
        "print(generate_text(unigram_demonstration_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq-WtaM6F6kN"
      },
      "source": [
        "In fact there are many strategies to get better-sounding samples, such as only sampling from the top-k words or sharpening the distribution with a temperature.  You can read more about sampling from a language model in this recent paper: https://arxiv.org/pdf/1904.09751.pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuopg4rYjf2O"
      },
      "source": [
        "You will need to submit some outputs from the models you implement for us to grade.  The following function will be used to generate the required output files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZB6MKPbm4z9s",
        "outputId": "2d682c2b-e20b-43c4-fae1-3ef8ac7f7f51",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-08 00:06:04--  https://cal-cs288.github.io/sp21/project_files/proj_1/eval_prefixes.txt\n",
            "Resolving cal-cs288.github.io (cal-cs288.github.io)... 185.199.110.153, 185.199.109.153, 185.199.111.153, ...\n",
            "Connecting to cal-cs288.github.io (cal-cs288.github.io)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 519055 (507K) [text/plain]\n",
            "Saving to: ‘eval_prefixes.txt.1’\n",
            "\n",
            "eval_prefixes.txt.1 100%[===================>] 506.89K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2026-02-08 00:06:04 (20.9 MB/s) - ‘eval_prefixes.txt.1’ saved [519055/519055]\n",
            "\n",
            "--2026-02-08 00:06:04--  https://cal-cs288.github.io/sp21/project_files/proj_1/eval_output_vocab.txt\n",
            "Resolving cal-cs288.github.io (cal-cs288.github.io)... 185.199.110.153, 185.199.109.153, 185.199.111.153, ...\n",
            "Connecting to cal-cs288.github.io (cal-cs288.github.io)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12497 (12K) [text/plain]\n",
            "Saving to: ‘eval_output_vocab.txt.1’\n",
            "\n",
            "eval_output_vocab.t 100%[===================>]  12.20K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2026-02-08 00:06:04 (3.34 MB/s) - ‘eval_output_vocab.txt.1’ saved [12497/12497]\n",
            "\n",
            "--2026-02-08 00:06:04--  https://cal-cs288.github.io/sp21/project_files/proj_1/eval_prefixes_short.txt\n",
            "Resolving cal-cs288.github.io (cal-cs288.github.io)... 185.199.110.153, 185.199.109.153, 185.199.111.153, ...\n",
            "Connecting to cal-cs288.github.io (cal-cs288.github.io)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 105976 (103K) [text/plain]\n",
            "Saving to: ‘eval_prefixes_short.txt.1’\n",
            "\n",
            "eval_prefixes_short 100%[===================>] 103.49K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2026-02-08 00:06:04 (10.9 MB/s) - ‘eval_prefixes_short.txt.1’ saved [105976/105976]\n",
            "\n",
            "--2026-02-08 00:06:05--  https://cal-cs288.github.io/sp21/project_files/proj_1/eval_output_vocab_short.txt\n",
            "Resolving cal-cs288.github.io (cal-cs288.github.io)... 185.199.110.153, 185.199.109.153, 185.199.111.153, ...\n",
            "Connecting to cal-cs288.github.io (cal-cs288.github.io)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3802 (3.7K) [text/plain]\n",
            "Saving to: ‘eval_output_vocab_short.txt.1’\n",
            "\n",
            "eval_output_vocab_s 100%[===================>]   3.71K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2026-02-08 00:06:05 (2.94 MB/s) - ‘eval_output_vocab_short.txt.1’ saved [3802/3802]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_prefixes.txt\n",
        "!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_output_vocab.txt\n",
        "!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_prefixes_short.txt\n",
        "!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_output_vocab_short.txt\n",
        "\n",
        "def save_truncated_distribution(model, filename, short=True):\n",
        "    \"\"\"Generate a file of truncated distributions.\n",
        "\n",
        "    Probability distributions over the full vocabulary are large,\n",
        "    so we will truncate the distribution to a smaller vocabulary.\n",
        "\n",
        "    Please do not edit this function\n",
        "    \"\"\"\n",
        "    vocab_name = 'eval_output_vocab'\n",
        "    prefixes_name = 'eval_prefixes'\n",
        "\n",
        "    if short:\n",
        "      vocab_name += '_short'\n",
        "      prefixes_name += '_short'\n",
        "\n",
        "    with open('{}.txt'.format(vocab_name), 'r') as eval_vocab_file:\n",
        "        eval_vocab = [w.strip() for w in eval_vocab_file]\n",
        "    # Map unknown words to <unk> token ID\n",
        "    unk_id = vocab.stoi['<unk>']\n",
        "    eval_vocab_ids = [vocab.stoi.get(s, unk_id) for s in eval_vocab]\n",
        "\n",
        "    all_selected_probabilities = []\n",
        "    with open('{}.txt'.format(prefixes_name), 'r') as eval_prefixes_file:\n",
        "        lines = eval_prefixes_file.readlines()\n",
        "        for line in tqdm.tqdm(lines, leave=False):\n",
        "            prefix = line.strip().split(' ')\n",
        "            probs = model.next_word_probabilities(prefix)\n",
        "            selected_probs = np.array([probs[i] for i in eval_vocab_ids], dtype=np.float32)\n",
        "            all_selected_probabilities.append(selected_probs)\n",
        "\n",
        "    all_selected_probabilities = np.stack(all_selected_probabilities)\n",
        "    np.save(filename, all_selected_probabilities)\n",
        "    print('saved', filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nzVrTWcH67Q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "save_truncated_distribution(unigram_demonstration_model, 'unigram_predictions.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEfUwbbS9vy0"
      },
      "source": [
        "### N-gram Model\n",
        "\n",
        "Now it's time to implement an n-gram language model.\n",
        "\n",
        "Because not every n-gram will have been observed in training, use add-alpha smoothing to make sure no output word has probability 0.\n",
        "\n",
        "$$P(w_2|w_1)=\\frac{C(w_1,w_2)+\\alpha}{C(w_1)+N\\alpha}$$\n",
        "\n",
        "where $N$ is the vocab size and $C$ is the count for the given bigram.  An alpha value around `3e-3`  should work.\n",
        "\n",
        "One edge case you will need to handle is at the beginning of the text where you don't have `n-1` prior words.  You can handle this however you like as long as you produce a valid probability distribution, but just using a uniform distribution over the vocabulary is reasonable for the purposes of this project.\n",
        "\n",
        "A properly implemented bi-gram model should get a perplexity below 550 on the validation set.\n",
        "\n",
        "**Note**: Do not change the signature of the `next_word_probabilities` and `perplexity` functions.  We will use these as a common interface for all of the different model types.  Make sure these two functions call `n_gram_probability`, because later we are going to override `n_gram_probability` in a subclass.\n",
        "Also, we suggest pre-computing and caching the counts $C$ when you initialize `NGramModel` for efficiency.\n",
        "\n",
        "**Deliverable**: Submit the bigram distribution from the Ngram model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGnGpnPIXpTW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "class NGramModel:\n",
        "    def __init__(self, train_text, n=2, alpha=3e-3):\n",
        "        # get counts and perform any other setup\n",
        "        self.n = n\n",
        "        self.smoothing = alpha\n",
        "\n",
        "        self.word_counts = Counter()\n",
        "        self.prefix_counts = Counter()\n",
        "        self.ngram_counts = Counter()\n",
        "        self.total_counts = len(train_text)\n",
        "\n",
        "        length = len(train_text)\n",
        "        for i in range(self.n, length+1):\n",
        "            ngram = tuple(train_text[i-n:i])\n",
        "            word = ngram[-1]\n",
        "            prefix = ngram[:-1]\n",
        "\n",
        "            self.ngram_counts[ngram] += 1\n",
        "            self.prefix_counts[prefix] += 1\n",
        "            self.word_counts[word] += 1\n",
        "\n",
        "\n",
        "    def probability(self, word: str):\n",
        "        return self.word_counts[word] / self.total_counts\n",
        "\n",
        "\n",
        "    def n_gram_probability(self, n_gram):\n",
        "        \"\"\"Return the probability of the last word in an n-gram.\n",
        "\n",
        "        n_gram -- a list of string tokens\n",
        "        returns the conditional probability of the last token given the rest.\n",
        "        \"\"\"\n",
        "        assert len(n_gram) == self.n\n",
        "        n_gram = tuple(n_gram)\n",
        "        prefix = n_gram[:-1]\n",
        "        return (self.ngram_counts[n_gram] + self.smoothing) / (self.prefix_counts[prefix] + len(vocab) * self.smoothing)\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        \"\"\"Return a list of probabilities for each word in the vocabulary.\"\"\"\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # use your function n_gram_probability\n",
        "        # vocab.itos contains a list of words to return probabilities for\n",
        "        l = len(text_prefix)\n",
        "        if l >= self.n - 1:\n",
        "            text_prefix = text_prefix[-(self.n-1):] if self.n > 1 else []\n",
        "        else:\n",
        "            text_prefix = ['<eos>'] * (self.n-1-l) + text_prefix\n",
        "        return list(self.n_gram_probability(text_prefix + [word]) for word in vocab.itos)\n",
        "\n",
        "    def perplexity(self, full_text):\n",
        "        \"\"\" full_text is a list of string tokens\n",
        "        return perplexity as a float \"\"\"\n",
        "        log_probabilities = []\n",
        "        for i, word in enumerate(full_text):\n",
        "            if i < self.n - 1:\n",
        "                prefix = ['<eos>'] * (self.n - 1 - i) + full_text[:i]\n",
        "            else:\n",
        "                prefix = full_text[i-self.n+1:i]\n",
        "\n",
        "            gram = prefix + [word]\n",
        "            prob = self.n_gram_probability(gram)\n",
        "            if prob == 0:\n",
        "                prob = 1e-10\n",
        "            log_probabilities.append(math.log(prob, 2))\n",
        "        return 2 ** -np.mean(log_probabilities)\n",
        "\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # use your function n_gram_probability\n",
        "        # This method should differ a bit from the example unigram model because\n",
        "        # the first n-1 words of full_text must be handled as a special case.\n",
        "\n",
        "\n",
        "unigram_model = NGramModel(train_text, 1)\n",
        "check_validity(unigram_model)\n",
        "print('unigram validation perplexity:', unigram_model.perplexity(validation_text)) # this should be the almost the same as our unigram model perplexity above\n",
        "\n",
        "bigram_model = NGramModel(train_text, n=2)\n",
        "check_validity(bigram_model)\n",
        "print('bigram validation perplexity:', bigram_model.perplexity(validation_text))\n",
        "\n",
        "trigram_model = NGramModel(train_text, n=3)\n",
        "check_validity(trigram_model)\n",
        "print('trigram validation perplexity:', trigram_model.perplexity(validation_text)) # this won't do very well...\n",
        "\n",
        "save_truncated_distribution(bigram_model, 'bigram_predictions.npy') # this might take a few minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzRRLnk73-r9"
      },
      "source": [
        "Please download `bigram_predictions.npy` once you finish this section so that you can submit it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6zgYw9VTx1"
      },
      "source": [
        "We can also generate samples from the model to get an idea of how it is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2V-qHxB4yhS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(generate_text(bigram_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsR8_Ch7AXAZ"
      },
      "source": [
        "We now free up some RAM, **it is important to run the cell below, otherwise you may quite possibly run out of RAM in the runtime.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjKt1ncf_ypz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Free up some RAM.\n",
        "del bigram_model\n",
        "del trigram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5Y0S6XbB1iZ"
      },
      "source": [
        "### Neural N-gram Model\n",
        "\n",
        "In this section, you will implement a neural version of an n-gram model.  The model will use a simple feedforward neural network that takes the previous `n-1` words and outputs a distribution over the next word.\n",
        "\n",
        "You will use PyTorch to implement the model.  We've provided a little bit of code to help with the data loading using PyTorch's data loaders (https://pytorch.org/docs/stable/data.html)\n",
        "\n",
        "A model with the following architecture and hyperparameters should reach a validation perplexity below 230.\n",
        "* embed the words with dimension 128, then flatten into a single embedding for $n-1$ words (with size $(n-1)*128$)\n",
        "* run 2 hidden layers with 1024 hidden units, then project down to size 128 before the final layer (ie. 4 layers total).\n",
        "* use weight tying for the embedding and final linear layer (this made a very large difference in our experiments); you can do this by creating the output layer with `nn.Linear`, then using `F.embedding` with the linear layer's `.weight` to embed the input\n",
        "* rectified linear activation (ReLU) and dropout 0.1 after first 2 hidden layers. **Note: You will likely find a performance drop if you add a nonlinear activation function after the dimension reduction layer.**\n",
        "* train for 10 epochs with the Adam optimizer (should take around 15-20 minutes)\n",
        "\n",
        "\n",
        "We encourage you to try other architectures and hyperparameters, and you will likely find some that work better than the ones listed above.  A proper implementation with these should be enough to receive full credit on the assignment, though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jokaz820Fk1h",
        "outputId": "fe33ddf2-0acf-4c83-f3c8-cf29e36f772c",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-944974359.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m \u001b[0mneural_trigram_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNGramModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_trigram_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0mneural_trigram_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-944974359.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n, lr, epochs, batch_size)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNGramNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \"\"\"\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \"\"\"\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "from IPython.terminal.embed import embed\n",
        "def ids(tokens):\n",
        "    return [vocab.stoi[t] for t in tokens]\n",
        "\n",
        "assert torch.cuda.is_available(), \"no GPU found; in Colab go to 'Edit->Notebook settings' and choose a GPU hardware accelerator; \\n in Kaggle go to 'Settings->Accelerator' and choose a GPU hardware accelerator\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class NeuralNgramDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text_token_ids, n):\n",
        "        self.text_token_ids = text_token_ids\n",
        "        self.n = n\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_token_ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if i < self.n-1:\n",
        "            prev_token_ids = [vocab.stoi['<eos>']] * (self.n-i-1) + self.text_token_ids[:i]\n",
        "        else:\n",
        "            prev_token_ids = self.text_token_ids[i-self.n+1:i]\n",
        "\n",
        "        assert len(prev_token_ids) == self.n-1\n",
        "\n",
        "        x = torch.tensor(prev_token_ids)\n",
        "        y = torch.tensor(self.text_token_ids[i])\n",
        "        return x, y\n",
        "\n",
        "class NeuralNGramNetwork(nn.Module):\n",
        "    # a PyTorch Module that holds the neural network for your model\n",
        "\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.n = n\n",
        "        self.embedding_dim = 128\n",
        "        self.hidden_dim = 1024\n",
        "        self.vocab_size = len(vocab)\n",
        "\n",
        "        # embed the words with dimension 128, then flatten into a single embedding for n-1 words (with size (n-1)*128)\n",
        "        # run 2 hidden layers with 1024 hidden units, then project down to size 128 before the final layer (ie. 4 layers total).\n",
        "        # use weight tying for the embedding and final linear layer\n",
        "        # rectified linear activation (ReLU) and dropout 0.1 after first 2 hidden layers.\n",
        "\n",
        "        # Output layer, whose weight will be tied with the embedding layer\n",
        "        self.output = nn.Linear(self.embedding_dim, self.vocab_size)\n",
        "\n",
        "        # Hidden layers\n",
        "        self.hidden1 = nn.Linear((n-1) * self.embedding_dim, self.hidden_dim)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.hidden2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.reduction = nn.Linear(self.hidden_dim, self.embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is a tensor of inputs with shape (batch, n-1)\n",
        "        # this function returns a tensor of log probabilities with shape (batch, vocab_size)\n",
        "\n",
        "        # Weight tying: use output layer's weight for embedding\n",
        "        # F.embedding expects input to be LongTensor\n",
        "        x = F.embedding(x.long(), self.output.weight)\n",
        "\n",
        "        # Flatten the embeddings for the n-1 words\n",
        "        x = x.view(x.size(0), -1) # Reshapes to (batch_size, (n-1)*embedding_dim)\n",
        "\n",
        "        # Apply hidden layers with ReLU and dropout\n",
        "        x = F.relu(self.hidden1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Project down to embedding_dim before final output\n",
        "        x = self.reduction(x)\n",
        "\n",
        "        # Final output layer\n",
        "        x = self.output(x)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "class NeuralNGramModel:\n",
        "    # a class that wraps NeuralNGramNetwork to handle training and evaluation\n",
        "    # it's ok if this doesn't work for unigram modeling\n",
        "    def __init__(self, n, lr=0.001, epochs=10, batch_size=128):\n",
        "        self.n = n\n",
        "        self.network = NeuralNGramNetwork(n).cuda()\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.lr)\n",
        "        self.criterion = nn.NLLLoss()\n",
        "\n",
        "    def train(self):\n",
        "        dataset = NeuralNgramDataset(ids(train_text), self.n)\n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        self.network.train() # Set network to training mode\n",
        "        for epoch in tqdm.tqdm(range(self.epochs), desc=\"Training Epochs\"):\n",
        "            total_loss = 0\n",
        "            for x_batch, y_batch in tqdm.tqdm(train_loader, leave=False, desc=f\"Epoch {epoch+1}\"):\n",
        "                x_batch = x_batch.to(device) # Move input to GPU\n",
        "                y_batch = y_batch.to(device) # Move target to GPU\n",
        "\n",
        "                self.optimizer.zero_grad() # Clear gradients\n",
        "                outputs = self.network(x_batch) # Forward pass\n",
        "                loss = self.criterion(outputs, y_batch) # Calculate loss\n",
        "                loss.backward() # Backward pass\n",
        "                self.optimizer.step() # Update weights\n",
        "                total_loss += loss.item()\n",
        "            # print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        self.network.eval() # Set network to evaluation mode\n",
        "        with torch.no_grad(): # Disable gradient calculation for inference\n",
        "            # Handle prefixes shorter than n-1, similar to NeuralNgramDataset\n",
        "            if len(text_prefix) < self.n - 1:\n",
        "                prev_token_ids = [vocab.stoi['<eos>']] * (self.n - 1 - len(text_prefix)) + ids(text_prefix)\n",
        "            else:\n",
        "                prev_token_ids = ids(text_prefix[-(self.n-1):])\n",
        "\n",
        "            x = torch.tensor(prev_token_ids, dtype=torch.long).unsqueeze(0).cuda() # Add batch dimension and move to GPU\n",
        "            outputs = self.network(x) # Get log probabilities\n",
        "            probabilities = torch.exp(outputs).squeeze(0).cpu().numpy() # Convert log probs to probabilities and to numpy\n",
        "        return probabilities.tolist()\n",
        "\n",
        "    def perplexity(self, text):\n",
        "        self.network.eval() # Set network to evaluation mode\n",
        "        dataset = NeuralNgramDataset(ids(text), self.n)\n",
        "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        total_log_prob = 0\n",
        "        total_words = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in tqdm.tqdm(data_loader, leave=False, desc=\"Calculating Perplexity\"):\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                outputs = self.network(x_batch) # Get log probabilities\n",
        "                # NLLLoss already computes -log(P(y|x)), so we sum it up\n",
        "                loss = self.criterion(outputs, y_batch)\n",
        "                total_log_prob += loss.item() * len(y_batch) # Multiply by batch size to get sum of log probs\n",
        "                total_words += len(y_batch)\n",
        "\n",
        "        # Perplexity = 2^(-(1/N) * sum(log2(P(word))))\n",
        "        # NLLLoss gives sum(-log_e(P(word))), so we have -total_log_prob\n",
        "        # To convert to log2, divide by log_e(2)\n",
        "        avg_neg_log_prob_base_e = total_log_prob / total_words\n",
        "        avg_neg_log_prob_base_2 = avg_neg_log_prob_base_e / math.log(2)\n",
        "        return 2 ** avg_neg_log_prob_base_2\n",
        "\n",
        "\n",
        "neural_trigram_model = NeuralNGramModel(3)\n",
        "check_validity(neural_trigram_model)\n",
        "neural_trigram_model.train()\n",
        "print('neural trigram validation perplexity:', neural_trigram_model.perplexity(validation_text))\n",
        "\n",
        "save_truncated_distribution(neural_trigram_model, 'neural_trigram_predictions.npy', short=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i11YQqPMjNUS"
      },
      "outputs": [],
      "source": [
        "\n",
        "save_truncated_distribution(neural_trigram_model, 'neural_trigram_predictions.npy', short=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t5PCZnkB1r5"
      },
      "source": [
        "Free up RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1yH0lGOB1-S",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Delete model we don't need.\n",
        "del neural_trigram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHTOfrCG8CRF"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Upload a submission with the following files to Gradescope:\n",
        "* Part1.ipynb (rename to match this exactly)\n",
        "* neural_trigram_predictions.npy\n",
        "* bigram_predictions.npy\n",
        "\n",
        "You can upload files individually or as part of a zip file, but if using a zip file be sure you are zipping the files directly and not a folder that contains them.\n",
        "\n",
        "Be sure to check the output of the autograder after it runs.  It should confirm that no files are missing and that the output files have the correct format.  Note that the test set perplexities shown by the autograder are on a completely different scale from your validation set perplexities due to truncating the distribution and selecting different text.  Don't worry if the values seem much worse."
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 5614701,
          "sourceId": 9276871,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30369,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}